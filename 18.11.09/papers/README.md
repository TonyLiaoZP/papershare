* [Multi-Head Attention with Disagreement Regularization](https://arxiv.org/pdf/1810.10183.pdf)
* [Why Self-Attention  A Targeted Evaluation of Neural Machine Translation Architectures](https://arxiv.org/pdf/1808.08946.pdf)
* [A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING](https://arxiv.org/pdf/1703.03130.pdf)
* [DiSAN:Directional Self-Attention Network for RNN/CNN-free Language Understanding](https://arxiv.org/pdf/1709.04696.pdf)
* [Deep Semantic Role Labeling with Self-Attention](https://arxiv.org/pdf/1712.01586.pdf)
* [Distance-based Self-Attention Network for Natural Language Inference](https://arxiv.org/pdf/1712.02047.pdf)
* [Reinforced Self-Attention Network/a Hybrid of Hard and Soft Attention for Sequence Modeling](https://arxiv.org/pdf/1801.10296.pdf)
* [Self-Attention with Relative Position Representations](https://arxiv.org/pdf/1803.02155.pdf)
* [Linguistically-Informed Self-Attention for Semantic Role Labeling](https://arxiv.org/pdf/1804.08199.pdf)
* [Multi-Level Structured Self-Attentions for Distantly Supervised Relation Extraction](https://arxiv.org/pdf/1809.00699.pdf)